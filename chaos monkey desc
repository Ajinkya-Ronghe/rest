Overview

Purpose: Analyze a microservices codebase, surface likely failure modes and security risks, and generate chaos-engineering experiments with optional AI assistance.
Modes: CLI for quick reports, Streamlit UI for interactive analysis, and a pluggable AI client targeting your GenAI Lab gateway.
Key Features

Architecture discovery: Detects services via Dockerfiles, dependency manifests, docker-compose, and Kubernetes manifests.
Heuristic findings: Flags common risks (root containers, missing HEALTHCHECK, exposed ports, hardcoded secrets, missing k8s probes/resources).
Chaos suggestions: Proposes per-service experiments (kill, CPU/memory pressure, latency, network partition, disk fill, credential rotation).
AI recommendations: Builds a context-rich prompt and queries your GenAI Lab endpoint for prioritized failures and fixes.







Challenges:
Microservice Complexity: Dozens of services, configs, and manifests make risk discovery slow and error‑prone.
Hidden Misconfigurations: Root containers, missing health checks, floating tags, and weak k8s settings often slip past reviews.
Secret Leakage: Hardcoded tokens in env/configs and compose files create silent security exposure.
Limited Chaos Adoption: Teams lack time and guidance to design meaningful, safe chaos experiments.
Siloed Knowledge: Architecture context lives in people/docs; onboarding and audits are inconsistent.
Reactive Posture: Failures are found in production instead of being probed early in CI/CD.

Business Opportunities:
Proactive Resilience: Auto-generate targeted chaos experiments to validate SLOs before incidents.
Security Posture Improvement: Early detection of leaked secrets and insecure defaults reduces breach risk.
Faster Audits & Compliance: One-click architecture summaries and findings accelerate SOC2/ISO/PCI evidence.
Developer Productivity: Self-serve insights reduce SRE bottlenecks and shorten onboarding to complex stacks.
Cost Optimization: Identify waste (e.g., missing limits, unnecessary exposures) and reduce incident-driven costs.
Platform Offerings: Package as an internal DevEx tool or external SaaS add-on for platform engineering.

Who Benefits:
SRE/Platform Teams: Automated checks and chaos recipes reduce toil and increase coverage.
Security/Compliance: Continuous scanning surfaces policy violations early with fix guidance.
App Teams: Clear, actionable fixes integrated into PR/CI context.
Leadership: Improved reliability metrics and auditability with lower incident frequency.
Differentiators







How the App Adheres (today)

Data minimization: build_ai_prompt samples only small snippets and limits total characters via a budget slider in the UI.
Control of outbound data: Offline mode (--offline) fully disables network; Streamlit enforces AI usage but still respects env-driven gateway and budgets.
Secret hygiene: Heuristics flag possible secrets in .env/configs before sending; gateway key required via AI_API_KEY env (never written to files).
User control: Include/exclude patterns supported in CLI; UI allows choosing the folder (defaults to the bundled sample).
Fail safe: Network and model errors return readable failures and keep the report usable.
Config & Controls You Can Enable

Redaction: Add masking prior to prompt assembly for high-risk keys (e.g., patterns like password|secret|token).
Allow/Deny Lists: Restrict which file types and paths can be sampled into prompts (tests, migrations, vendor dirs excluded by default).
Prompt Budgets: Keep sample_text_budget conservative; lower it for sensitive repos.
Network Boundaries: Point AI_API_BASE to your internal GenAI gateway; set AI_SSL_NO_VERIFY=0 in production; log mTLS at the proxy.
Access Control: Run the app under least-privilege and read-only repo access in CI; rotate AI_API_KEY regularly.
Risks & Mitigations

Leakage Risk: Prompts may include secrets if present in code. Mitigate with redaction, allowlists, and pre-scan secret checks (already included).
Hallucination/Overreach: AI suggestions may be incorrect. Keep human-in-the-loop review and tie advice to evidence lines.
Vendor Lock/Policy Drift: Gateway constraints or model renames. The client handles model aliasing and fails gracefully; keep policies pinned and monitored.
Compliance (PII/IP): Only scan repos you’re authorized to scan; avoid uploading proprietary data beyond approved gateways.
Operational Practices

Logging: Log actions and parameters (not content) for audit; avoid logging keys or prompt contents.
Retention: Do not store AI responses by default; user explicitly downloads reports if desired.
Testing & Evals: Keep unit tests for redaction and secret detection; periodically red-team prompts with seeded secrets to validate guardrails.
Documentation: State AI usage, data flow, and limitations in README and user docs; provide a change log for model/endpoint changes.
Next Enhancements

Add redaction middleware for prompts with configurable patterns.
Add an allowlist/denylist config file for promptable paths.
Provide a SARIF output for CI with severity gating and policy exceptions.
Add privacy mode that disables code snippets and uses only metadata.
Add automated DSR support doc and data flow diagram for governance packs.
